{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spotipy\n",
      "  Downloading spotipy-2.17.1-py3-none-any.whl (26 kB)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.25.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 4.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.15.0 in /home/afeka/anaconda3/lib/python3.8/site-packages (from spotipy) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/afeka/anaconda3/lib/python3.8/site-packages (from requests>=2.25.0->spotipy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/afeka/anaconda3/lib/python3.8/site-packages (from requests>=2.25.0->spotipy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/afeka/anaconda3/lib/python3.8/site-packages (from requests>=2.25.0->spotipy) (2020.6.20)\n",
      "\u001b[31mERROR: botocore 1.18.4 has requirement urllib3<1.26,>=1.20; python_version != \"3.4\", but you'll have urllib3 1.26.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: urllib3, requests, spotipy\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed requests-2.25.1 spotipy-2.17.1 urllib3-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting confluent-kafka\n",
      "  Downloading confluent_kafka-1.6.1-cp38-cp38-manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: confluent-kafka\n",
      "Successfully installed confluent-kafka-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, Producer\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from math import sqrt\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine_similarity\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "SPOTIPY_CLIENT_ID='ff464bf1719f4878a4c6b078329878fb'\n",
    "SPOTIPY_CLIENT_SECRET='ccd5c137ece94d78b801a283b50f0340'\n",
    "auth_manager = SpotifyClientCredentials(client_id=SPOTIPY_CLIENT_ID,client_secret=SPOTIPY_CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getrows(df, rownums=None):\n",
    "    return df.rdd.zipWithIndex().filter(lambda x: x[1] > rownums[0] and x[1] < rownums[1]).map(lambda x: x[0])\n",
    "\n",
    "def similarity_measure(record):\n",
    "    prediction = record['prediction']\n",
    "    #songs features\n",
    "    try:\n",
    "        features_vec = numpy.asarray(list(record['features'])).reshape(1, -1)\n",
    "        cluster_info = spark \\\n",
    "                    .sql(f'SELECT feature_one, feature_two, track_name, artist_name FROM Predictions WHERE prediction={prediction}') \\\n",
    "                    .toPandas()\n",
    "        cluster_vec = numpy.asarray(cluster_info[['feature_one', 'feature_two']])\n",
    "        output = cosine_similarity(features_vec, cluster_vec).T\n",
    "        cluster_info['similarity'] = pd.Series(output.tolist()).values\n",
    "    \n",
    "        sim_DF = spark.createDataFrame(cluster_info)\n",
    "        recommed = sim_DF.sort(sim_DF.similarity.desc()).select(['track_name', 'artist_name']).take(10)\n",
    "        return recommend\n",
    "    \n",
    "    finally:\n",
    "        \n",
    "        return 0\n",
    "                \n",
    "def read_and_normalize(streamData):\n",
    "    #Use API to get track features\n",
    "    streamData_features = sc.parallelize(streamData) \\\n",
    "                          .map(lambda track: (sp.audio_features(track[0].strip())[0], track[0], track[1])).collect()\n",
    "    \n",
    "    #For each track extract the relevant data\n",
    "    cols = ['danceability','energy','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo']\n",
    "    for i, (features, uri, name) in enumerate(streamData_features):\n",
    "        features = {column: value for (column, value) in features.items() if column in cols}\n",
    "        features['track_uri'] = uri\n",
    "        features['track_name'] = name\n",
    "        streamData_features[i] = features\n",
    "   \n",
    "    #return the to to RDD\n",
    "    streamData_RDD = sc.parallelize(streamData_features).persist()\n",
    "    \n",
    "    #Extract the values from each feature and normalize them\n",
    "    extract_RDD = streamData_RDD.map(lambda record: tuple(val for val in list(record.values()))).persist()\n",
    "    normal_streamData_RDD = extract_RDD.map(lambda x: [x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],float(x[8])/100,x[9],x[10]]) \\\n",
    "                        .map(lambda x: ([float(i/np.linalg.norm(x[:-2])) for i in x[:-2]], tuple(x[-2:]))) \\\n",
    "                        .map(lambda x: (*x[0], x[1][0], x[1][1] ))\n",
    "    \n",
    "    #Build a Dataframe from the normalized RDD\n",
    "    schema = StructType([\n",
    "    StructField('danceability', FloatType(), False),\n",
    "    StructField('energy', FloatType(), False),\n",
    "    StructField('loudness', FloatType(), False),\n",
    "    StructField('speechiness', FloatType(), False),\n",
    "    StructField('acousticness', FloatType(), False),\n",
    "    StructField('instrumentalness', FloatType(), False),\n",
    "    StructField('liveness', FloatType(), False),\n",
    "    StructField('valence', FloatType(), False),\n",
    "    StructField('tempo', FloatType(), False),\n",
    "    StructField('track_uri', StringType(), False),\n",
    "    StructField('track_name', StringType(), False)\n",
    "    ])\n",
    "    normal_streamData_DF = normal_streamData_RDD.map(lambda x: x).toDF(schema=schema)\n",
    "    normal_streamData_feature = normal_streamData_DF.rdd \\\n",
    "                                .map(lambda row: Row(Vectors.dense([row[0], row[1], row[2], row[3], row[4],row[5],row[6],row[7],row[8]]), row[9], row[10])).toDF([\"features\", \"track_uri\", \"track_name\"]).persist()\n",
    "    return normal_streamData_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train data to Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('index', StringType(), False),\n",
    "    StructField('danceability', DoubleType(), False),\n",
    "    StructField('energy', DoubleType(), False),\n",
    "    StructField('key', IntegerType(), False),\n",
    "    StructField('loudness', DoubleType(), False),\n",
    "    StructField('mode', IntegerType(), False),\n",
    "    StructField('speechiness', DoubleType(), False),\n",
    "    StructField('acousticness', DoubleType(), False),\n",
    "    StructField('instrumentalness', DoubleType(), False),\n",
    "    StructField('liveness', DoubleType(), False),\n",
    "    StructField('valence', DoubleType(), False),\n",
    "    StructField('tempo', DoubleType(), False),\n",
    "    StructField('type', StringType(), False),\n",
    "    StructField('id', StringType(), False),\n",
    "    StructField('uri', StringType(), False),\n",
    "    StructField('track_href', StringType(), False),\n",
    "    StructField('analysis_url', StringType(), False),\n",
    "    StructField('duration_ms', IntegerType(), False),\n",
    "    StructField('time_signature', IntegerType(), False),\n",
    "    StructField('popularity', IntegerType(), False),\n",
    "    StructField('track_name', StringType(), False),\n",
    "    StructField('artist_name', StringType(), False),\n",
    "])\n",
    "\n",
    "tracks_df = spark.read.csv(\"sp_dataset.csv\",schema=schema)\n",
    "existing = tracks_df.columns\n",
    "new_names = ['index','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','type','id','uri','track_href','analysis_url','duration_ms','time_signature','popularity','track_name','artist_name']\n",
    "for old , new in zip(existing,new_names):\n",
    "    tracks_df = tracks_df.withColumnRenamed(old,new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = getrows(tracks_df,[1,11000]).toDF()\n",
    "audio_trainRDD = train_df.rdd.map(lambda x: (x['danceability'],x['energy'],x['key'],x['loudness'],x['mode'],x['speechiness'],x['acousticness'],x['instrumentalness'],x['liveness'],x['valence'],x['tempo']))\n",
    "# divide tempo by 100 then normalize the data\n",
    "audio_normal_trainRDD = audio_trainRDD.map(lambda x: (x[0],x[1],x[3],x[5],x[6],x[7],x[8],x[9],float(x[10])/100))\\\n",
    "                       .map(lambda x: [float(i/np.linalg.norm(x)) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_namesRDD = train_df.rdd.map(lambda track: (track['uri'], track['track_name'], track['artist_name']))\n",
    "track_names = track_namesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('danceability', FloatType(), False),\n",
    "    StructField('energy', FloatType(), False),\n",
    "    StructField('loudness', FloatType(), False),\n",
    "    StructField('speechiness', FloatType(), False),\n",
    "    StructField('acousticness', FloatType(), False),\n",
    "    StructField('instrumentalness', FloatType(), False),\n",
    "    StructField('liveness', FloatType(), False),\n",
    "    StructField('valence', FloatType(), False),\n",
    "    StructField('tempo', FloatType(), False),\n",
    "])\n",
    "\n",
    "audio_normal_train_DF = audio_normal_trainRDD.map(lambda x: x).toDF(schema=schema)\n",
    "normal_train_df = audio_normal_train_DF.rdd.map(lambda row: Row(Vectors.dense([row[0], row[1], row[2], row[3], row[4],row[5],row[6],row[7],row[8]]))).toDF([\"features\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement PCA on the train data to extract 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData ,testData = my_pca(normal_train_df, normal_test_df, 2)\n",
    "pca = PCA(k=2, inputCol = 'features' , outputCol = 'pcaFeatures')\n",
    "pca_model = pca.fit(normal_train_df)\n",
    "trainData = pca_model.transform(normal_train_df).select(\"pcaFeatures\")\n",
    "trainData = trainData.rdd.map(lambda row: row).toDF([\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on Kmeans Algorithmn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(6).setSeed(1)\n",
    "kmeans_model = kmeans.fit(trainData)\n",
    "predictionsTrain = kmeans_model.transform(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_RDD = predictionsTrain.rdd.zip(track_namesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(features=DenseVector([0.1822, -0.0098]), prediction=1),\n",
       "  ('spotify:track:4mKjZaJd7w1UEeGOqHCcof', 'True Love', 'WE ARE TWIN'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_DF = zipped_RDD.map(lambda row: (row[1][0],row[1][1],row[1][2],float(row[0][0][0]),float(row[0][0][1]),row[0][1])).toDF(['uri','track_name','artist_name','feature_one','feature_two','prediction'])\n",
    "#Create SQl table for later use\n",
    "zipped_DF.createOrReplaceTempView(\"Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "\n",
    "conf = {'bootstrap.servers': \"localhost:9092\",\n",
    "        'group.id': \"spotify\",\n",
    "        'auto.offset.reset': 'smallest'}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "batch_size = 0\n",
    "batch_msg = []\n",
    "batch_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For batch id 1 ----------------------\n",
      "For batch id 2 ----------------------\n",
      "For batch id 3 ----------------------\n",
      "For batch id 4 ----------------------\n",
      "For batch id 5 ----------------------\n",
      "For batch id 6 ----------------------\n",
      "For batch id 7 ----------------------\n",
      "For batch id 8 ----------------------\n",
      "For batch id 9 ----------------------\n",
      "For batch id 10 ----------------------\n",
      "For batch id 11 ----------------------\n",
      "For batch id 12 ----------------------\n",
      "For batch id 13 ----------------------\n",
      "For batch id 14 ----------------------\n",
      "For batch id 15 ----------------------\n",
      "For batch id 16 ----------------------\n",
      "For batch id 17 ----------------------\n",
      "For batch id 18 ----------------------\n",
      "For batch id 19 ----------------------\n",
      "For batch id 20 ----------------------\n",
      "For batch id 21 ----------------------\n",
      "For batch id 22 ----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-de9cee95087b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    consumer.subscribe(['spotify-recommend_v1'])\n",
    "\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        \n",
    "        if msg is None: \n",
    "            continue\n",
    "\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if msg.key().decode(\"utf-8\") == '0':\n",
    "                    print(msg.value().decode(\"utf-8\").split(\" \"))\n",
    "                    break\n",
    "                    \n",
    "            if batch_size < 10:\n",
    "                batch_size += 1\n",
    "                \n",
    "                msg = msg.value().decode(\"utf-8\").split(\",\")\n",
    "                batch_msg.append(tuple(msg))\n",
    "            else:\n",
    "                normal_streamData_feature = read_and_normalize(batch_msg)\n",
    "                #compress the data into 2 features only and run on Kmeans\n",
    "                pca_data = pca_model.transform(normal_streamData_feature.select(\"features\")).select(\"pcaFeatures\")\n",
    "                to_kmeans_data = pca_data.rdd.map(lambda row: row).toDF([\"features\"])\n",
    "\n",
    "                stream_predictions = kmeans_model.transform(to_kmeans_data)\n",
    "\n",
    "                #Zip \n",
    "                track_info = normal_streamData_feature.select([\"track_uri\", \"track_name\"])\n",
    "                schema = StructType(stream_predictions.schema.fields + track_info.schema.fields)\n",
    "                combined_data = stream_predictions.rdd.zip(track_info.rdd).map(lambda x: x[0]+x[1]).toDF(schema)\n",
    "                combined_data = combined_data.withColumn('recommendations', lit(0)).collect()\n",
    "                \n",
    "                #Add similarity to stream predoctions\n",
    "                all_recommendations = []\n",
    "                for row in combined_data:\n",
    "                    recommendations = similarity_measure(row)\n",
    "                    if recommendations != 0:\n",
    "                        new_row = Row(track_name=row['track_name'], recommend=recommendations, track_uri=row['track_uri'])\n",
    "                        all_recommendations.append(new_row)\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                \n",
    "                print(f'For batch id {batch_id} ----------------------')\n",
    "                for recm in all_recommendations:\n",
    "                    t_name = recm['track_name']\n",
    "                    recm_lst = recm['recommend']\n",
    "                    msg = \" \".join(f'{recm_lst}')\n",
    "                    p.produce(\"recommend-result\", key = f'{batch_id},{t_name}', value = msg)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                batch_id += 1\n",
    "                batch_size = 0\n",
    "                batch_msg = []\n",
    "finally:\n",
    "    p.produce(\"recommend-result\", key = 'stop', value = '0')\n",
    "    #Close down consumer to commit final offsets.\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schema = StructType([StructField('track_uri', StringType(), False),StructField('track_name', StringType(), False)])\n",
    "test_data = spark.read.format(\"csv\").schema(test_schema).load(\"DataToStream.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spotify:track:0xBOAziRHVjPXOsigGpPHh',\n",
       "  'How to Save a Life (Running Mix) [140 BPM]'),\n",
       " ('spotify:track:25eA6wrjKEVtJb2LGt9ii3',\n",
       "  \"I Don't Wanna Miss a Thing (Runner Remix)\"),\n",
       " ('spotify:track:3XDHBRyJQWx1zCGhdsQRP4', 'Basket Case - Workout Remix'),\n",
       " ('spotify:track:6sumVx6Qrsnerf4KRvA4Va',\n",
       "  'Only Girl (In the World) [Running Mix] [140 BPM]'),\n",
       " ('spotify:track:1q6JBBjlE4dTGKzntIZS2n',\n",
       "  'Body Like a Back Road - Workout Mix'),\n",
       " ('spotify:track:5qpLBTpYlaBdSMRRhfvWfw', 'This Is How We Roll - Workout Mix'),\n",
       " ('spotify:track:5aU4XjyMB9eFw67lGSrSQ3', 'H.O.L.Y. - Workout Mix'),\n",
       " ('spotify:track:1KJq03F5FI4SuplXCIJaWD', 'Better Man - Workout Mix'),\n",
       " ('spotify:track:4XQ0WbhvqOZG33PqO6UEty', 'Homegrown - Workout Mix'),\n",
       " ('spotify:track:6R5wnsCBZZRDxE0P1FCGsk',\n",
       "  'Setting the World on Fire - Workout Mix')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamData = test_data.rdd.map(lambda s: (s['track_uri'], s['track_name'])).collect()[:10]\n",
    "streamData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use API to get track features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamData_features = sc.parallelize(streamData).map(lambda track: (sp.audio_features(track[0])[0], track[0], track[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'danceability': 0.675,\n",
       "  'energy': 0.99,\n",
       "  'key': 6,\n",
       "  'loudness': -7.654,\n",
       "  'mode': 1,\n",
       "  'speechiness': 0.0463,\n",
       "  'acousticness': 0.0862,\n",
       "  'instrumentalness': 0.0159,\n",
       "  'liveness': 0.341,\n",
       "  'valence': 0.645,\n",
       "  'tempo': 140.002,\n",
       "  'type': 'audio_features',\n",
       "  'id': '0xBOAziRHVjPXOsigGpPHh',\n",
       "  'uri': 'spotify:track:0xBOAziRHVjPXOsigGpPHh',\n",
       "  'track_href': 'https://api.spotify.com/v1/tracks/0xBOAziRHVjPXOsigGpPHh',\n",
       "  'analysis_url': 'https://api.spotify.com/v1/audio-analysis/0xBOAziRHVjPXOsigGpPHh',\n",
       "  'duration_ms': 305787,\n",
       "  'time_signature': 4},\n",
       " 'spotify:track:0xBOAziRHVjPXOsigGpPHh',\n",
       " 'How to Save a Life (Running Mix) [140 BPM]')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamData_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each track extract the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = audio_normal_train_DF.columns\n",
    "for i, (features, uri, name) in enumerate(streamData_features):\n",
    "    features = {column: value for (column, value) in features.items() if column in cols}\n",
    "    features['track_uri'] = uri\n",
    "    features['track_name'] = name\n",
    "    streamData_features[i] = features\n",
    "   \n",
    "\n",
    "\n",
    "#return the to to RDD\n",
    "streamData_RDD = sc.parallelize(streamData_features).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'danceability': 0.675,\n",
       "  'energy': 0.99,\n",
       "  'loudness': -7.654,\n",
       "  'speechiness': 0.0463,\n",
       "  'acousticness': 0.0862,\n",
       "  'instrumentalness': 0.0159,\n",
       "  'liveness': 0.341,\n",
       "  'valence': 0.645,\n",
       "  'tempo': 140.002,\n",
       "  'track_uri': 'spotify:track:0xBOAziRHVjPXOsigGpPHh',\n",
       "  'track_name': 'How to Save a Life (Running Mix) [140 BPM]'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamData_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the values from each feature and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_RDD = streamData_RDD.map(lambda record: tuple(val for val in list(record.values()))).persist()\n",
    "normal_streamData_RDD = extract_RDD.map(lambda x: [x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],float(x[8])/100,x[9],x[10]]) \\\n",
    "                        .map(lambda x: ([float(i/np.linalg.norm(x[:-2])) for i in x[:-2]], tuple(x[-2:]))) \\\n",
    "                        .map(lambda x: (*x[0], x[1][0], x[1][1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.08536672628476583,\n",
       "  0.1252045318843232,\n",
       "  -0.9679954414571815,\n",
       "  0.005855525077014307,\n",
       "  0.010901647119624908,\n",
       "  0.002010860663596706,\n",
       "  0.043126005426822434,\n",
       "  0.08157264956099845,\n",
       "  0.1770594431602931,\n",
       "  'spotify:track:0xBOAziRHVjPXOsigGpPHh',\n",
       "  'How to Save a Life (Running Mix) [140 BPM]')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_streamData_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Dataframe from the normalized RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('danceability', FloatType(), False),\n",
    "    StructField('energy', FloatType(), False),\n",
    "    StructField('loudness', FloatType(), False),\n",
    "    StructField('speechiness', FloatType(), False),\n",
    "    StructField('acousticness', FloatType(), False),\n",
    "    StructField('instrumentalness', FloatType(), False),\n",
    "    StructField('liveness', FloatType(), False),\n",
    "    StructField('valence', FloatType(), False),\n",
    "    StructField('tempo', FloatType(), False),\n",
    "    StructField('track_uri', StringType(), False),\n",
    "    StructField('track_name', StringType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "normal_streamData_DF = normal_streamData_RDD.map(lambda x: x).toDF(schema=schema)\n",
    "normal_streamData_feature = normal_streamData_DF.rdd.map(lambda row: Row(Vectors.dense([row[0], row[1], row[2], row[3], row[4],row[5],row[6],row[7],row[8]]), row[9], row[10])).toDF([\"features\", \"track_uri\", \"track_name\"]).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA on the streamed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress the data into 2 features only and run on Kmeans\n",
    "pca_data = pca_model.transform(normal_streamData_feature.select(\"features\")).select(\"pcaFeatures\")\n",
    "to_kmeans_data = pca_data.rdd.map(lambda row: row).toDF([\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the to Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_predictions = kmeans_model.transform(to_kmeans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>features</th><th>prediction</th></tr>\n",
       "<tr><td>[0.14088983892811...</td><td>2</td></tr>\n",
       "<tr><td>[0.09532336953450...</td><td>1</td></tr>\n",
       "<tr><td>[0.12267709572253...</td><td>2</td></tr>\n",
       "<tr><td>[0.21706694050642...</td><td>0</td></tr>\n",
       "<tr><td>[0.12424063091373...</td><td>2</td></tr>\n",
       "<tr><td>[0.11195274526271...</td><td>2</td></tr>\n",
       "<tr><td>[0.09217066861381...</td><td>5</td></tr>\n",
       "<tr><td>[0.02047888859523...</td><td>5</td></tr>\n",
       "<tr><td>[0.11943586201788...</td><td>2</td></tr>\n",
       "<tr><td>[0.10809742419870...</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----------+\n",
       "|            features|prediction|\n",
       "+--------------------+----------+\n",
       "|[0.14088983892811...|         2|\n",
       "|[0.09532336953450...|         1|\n",
       "|[0.12267709572253...|         2|\n",
       "|[0.21706694050642...|         0|\n",
       "|[0.12424063091373...|         2|\n",
       "|[0.11195274526271...|         2|\n",
       "|[0.09217066861381...|         5|\n",
       "|[0.02047888859523...|         5|\n",
       "|[0.11943586201788...|         2|\n",
       "|[0.10809742419870...|         1|\n",
       "+--------------------+----------+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_info = normal_streamData_feature.select([\"track_uri\", \"track_name\"])\n",
    "schema = StructType(stream_predictions.schema.fields + track_info.schema.fields)\n",
    "combined_data = stream_predictions.rdd.zip(track_info.rdd).map(lambda x: x[0]+x[1]).toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.withColumn('recommendations', lit(0)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([0.1409, 0.0507]), prediction=2, track_uri='spotify:track:0xBOAziRHVjPXOsigGpPHh', track_name='How to Save a Life (Running Mix) [140 BPM]', recommendations=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recommendations = []\n",
    "for row in combined_data:\n",
    "    recommendations = similarity_measure(row)\n",
    "    new_row = Row(track_name=row['track_name'], recommend=recommendations, track_uri=row['track_uri'])\n",
    "    all_recommendations.append(new_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
